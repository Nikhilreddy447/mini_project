{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Data importing"
      ],
      "metadata": {
        "id": "GfuUws12KvpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the uploaded .zip file\n",
        "zip_file_path = '/content/drive/MyDrive/data.zip'\n",
        "\n",
        "# Directory where you want to extract the files\n",
        "extract_dir = '/content/data_set'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "if not os.path.exists(extract_dir):\n",
        "    os.makedirs(extract_dir)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"Extraction complete!\")\n"
      ],
      "metadata": {
        "id": "7WxlbiNtJz7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1acce28-4bd8-4e80-a498-a9ce7eb9fcd5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing required Libraries"
      ],
      "metadata": {
        "id": "tdUT0FCCpbCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cD-ApKRSF_K0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50, VGG16, InceptionV3, MobileNetV2, DenseNet121, EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing"
      ],
      "metadata": {
        "id": "a-x05xsvpmqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TEipkGbrg3oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/data_set/data'"
      ],
      "metadata": {
        "id": "h07TkqVho5Il"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentaion for training and testing data"
      ],
      "metadata": {
        "id": "1Xz7PBX2p8zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=0.2  # Splitting data into training and validation\n",
        ")"
      ],
      "metadata": {
        "id": "60icy2Fco9VE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation for validation and test sets\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)"
      ],
      "metadata": {
        "id": "sCwGoCF-qFkk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train generator\n",
        "print(\"Image for training dataset\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "print()\n",
        "# Test generator\n",
        "print(\"Images for validation dataset\")\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z948mDA6qKJS",
        "outputId": "7ca31845-2337-4b1b-fffd-06bf18190b89"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image for training dataset\n",
            "Found 3200 images belonging to 10 classes.\n",
            "\n",
            "Images for validation dataset\n",
            "Found 800 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Building and training"
      ],
      "metadata": {
        "id": "a5TBMpwaqp_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function which takes different base models and train them."
      ],
      "metadata": {
        "id": "DYLIsKtbqv39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(base_model):\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    predictions = Dense(10, activation='softmax')(x)  # predictions are 10 because there are 10 classes of data\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False  # ignoring training for the base model layers\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "yZLqwzh2qVhI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of base models used for this project"
      ],
      "metadata": {
        "id": "n8VAqUK9rOlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = [\n",
        "    ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "    VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "    InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "    MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "    DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "    EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "]"
      ],
      "metadata": {
        "id": "z8dfme9hrK-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553357ec-066c-4ae7-b6b5-737e06556742"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to store best model and accuracy\n",
        "best_model = None\n",
        "best_accuracy = 0.0"
      ],
      "metadata": {
        "id": "2Ry99Mt7rV_-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iterating through each model and traing it and updating the variable best_model when a best accuracy comes"
      ],
      "metadata": {
        "id": "4TqTShl9rn8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for base_model in base_models:\n",
        "    model = create_model(base_model)\n",
        "    model_name = base_model.name\n",
        "\n",
        "    # Define callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ModelCheckpoint(f'{model_name}.h5', monitor='val_loss', save_best_only=True)\n",
        "    ]\n",
        "    print(f'Training model: {model_name}')\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_accuracy = model.evaluate(validation_generator)\n",
        "\n",
        "    # Update the best model\n",
        "    if val_accuracy > best_accuracy:\n",
        "        best_accuracy = val_accuracy\n",
        "        best_model = model_name\n",
        "\n",
        "print(f'Best model: {best_model} with accuracy: {best_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H25ZRuwrmm0",
        "outputId": "5b22e11a-659b-44c1-d1e5-383a0b894dff"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model: resnet50\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - ETA: 0s - loss: 2.3235 - accuracy: 0.1541"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 112s 1s/step - loss: 2.3235 - accuracy: 0.1541 - val_loss: 2.2190 - val_accuracy: 0.1238\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 99s 990ms/step - loss: 2.1487 - accuracy: 0.2244 - val_loss: 2.1134 - val_accuracy: 0.2025\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 102s 1s/step - loss: 2.0871 - accuracy: 0.2387 - val_loss: 2.0661 - val_accuracy: 0.2325\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 104s 1s/step - loss: 2.0233 - accuracy: 0.2625 - val_loss: 2.0970 - val_accuracy: 0.2738\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 101s 1s/step - loss: 2.0051 - accuracy: 0.2784 - val_loss: 1.9977 - val_accuracy: 0.2525\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 100s 997ms/step - loss: 1.9655 - accuracy: 0.2891 - val_loss: 1.9852 - val_accuracy: 0.2812\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 101s 1s/step - loss: 1.9670 - accuracy: 0.2869 - val_loss: 1.9906 - val_accuracy: 0.2812\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 98s 982ms/step - loss: 1.9620 - accuracy: 0.2925 - val_loss: 2.0171 - val_accuracy: 0.2800\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 102s 1s/step - loss: 1.9305 - accuracy: 0.3131 - val_loss: 1.9353 - val_accuracy: 0.2912\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 100s 1s/step - loss: 1.9261 - accuracy: 0.3116 - val_loss: 1.9279 - val_accuracy: 0.2887\n",
            "25/25 [==============================] - 14s 534ms/step - loss: 1.9279 - accuracy: 0.2887\n",
            "Training model: vgg16\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 107s 990ms/step - loss: 1.6617 - accuracy: 0.4522 - val_loss: 1.2480 - val_accuracy: 0.5587\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 100s 993ms/step - loss: 1.1959 - accuracy: 0.5938 - val_loss: 1.1666 - val_accuracy: 0.6012\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 98s 979ms/step - loss: 1.0509 - accuracy: 0.6400 - val_loss: 0.9640 - val_accuracy: 0.6662\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 102s 1s/step - loss: 0.9558 - accuracy: 0.6750 - val_loss: 0.8962 - val_accuracy: 0.6888\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 100s 998ms/step - loss: 0.8933 - accuracy: 0.6928 - val_loss: 0.8466 - val_accuracy: 0.6988\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 105s 1s/step - loss: 0.8328 - accuracy: 0.7128 - val_loss: 0.7681 - val_accuracy: 0.7437\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 100s 1s/step - loss: 0.7917 - accuracy: 0.7247 - val_loss: 0.7280 - val_accuracy: 0.7575\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 103s 1s/step - loss: 0.7536 - accuracy: 0.7409 - val_loss: 0.7654 - val_accuracy: 0.7200\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 101s 1s/step - loss: 0.7243 - accuracy: 0.7509 - val_loss: 0.7250 - val_accuracy: 0.7725\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 101s 1s/step - loss: 0.6728 - accuracy: 0.7716 - val_loss: 0.7700 - val_accuracy: 0.7225\n",
            "25/25 [==============================] - 14s 549ms/step - loss: 0.7700 - accuracy: 0.7225\n",
            "Training model: inception_v3\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 108s 1s/step - loss: 1.4304 - accuracy: 0.5853 - val_loss: 0.7848 - val_accuracy: 0.7312\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 107s 1s/step - loss: 0.7746 - accuracy: 0.7359 - val_loss: 0.6022 - val_accuracy: 0.7912\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 102s 1s/step - loss: 0.6800 - accuracy: 0.7591 - val_loss: 0.5331 - val_accuracy: 0.8275\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 100s 999ms/step - loss: 0.6043 - accuracy: 0.7975 - val_loss: 0.5698 - val_accuracy: 0.8050\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 100s 1s/step - loss: 0.5471 - accuracy: 0.8053 - val_loss: 0.5027 - val_accuracy: 0.8250\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 100s 1s/step - loss: 0.5112 - accuracy: 0.8178 - val_loss: 0.4661 - val_accuracy: 0.8550\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 101s 1s/step - loss: 0.4970 - accuracy: 0.8256 - val_loss: 0.4666 - val_accuracy: 0.8375\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 98s 983ms/step - loss: 0.4564 - accuracy: 0.8450 - val_loss: 0.4593 - val_accuracy: 0.8525\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.4683 - accuracy: 0.8363 - val_loss: 0.4151 - val_accuracy: 0.8675\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 103s 1s/step - loss: 0.4198 - accuracy: 0.8544 - val_loss: 0.3945 - val_accuracy: 0.8625\n",
            "25/25 [==============================] - 14s 538ms/step - loss: 0.3945 - accuracy: 0.8625\n",
            "Training model: mobilenetv2_1.00_224\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.9992 - accuracy: 0.6769 - val_loss: 0.5785 - val_accuracy: 0.8062\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 98s 969ms/step - loss: 0.5557 - accuracy: 0.8112 - val_loss: 0.4870 - val_accuracy: 0.8350\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 99s 992ms/step - loss: 0.4694 - accuracy: 0.8409 - val_loss: 0.6091 - val_accuracy: 0.7962\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 98s 982ms/step - loss: 0.4499 - accuracy: 0.8453 - val_loss: 0.4577 - val_accuracy: 0.8575\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 103s 1s/step - loss: 0.4013 - accuracy: 0.8537 - val_loss: 0.3981 - val_accuracy: 0.8763\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 99s 995ms/step - loss: 0.3477 - accuracy: 0.8784 - val_loss: 0.4768 - val_accuracy: 0.8438\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 100s 1s/step - loss: 0.3625 - accuracy: 0.8750 - val_loss: 0.3772 - val_accuracy: 0.8875\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 99s 991ms/step - loss: 0.3094 - accuracy: 0.8881 - val_loss: 0.4146 - val_accuracy: 0.8700\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 102s 1s/step - loss: 0.3038 - accuracy: 0.8863 - val_loss: 0.3934 - val_accuracy: 0.8750\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 97s 969ms/step - loss: 0.2942 - accuracy: 0.8931 - val_loss: 0.3805 - val_accuracy: 0.8813\n",
            "25/25 [==============================] - 14s 570ms/step - loss: 0.3805 - accuracy: 0.8813\n",
            "Training model: densenet121\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 114s 1s/step - loss: 0.9057 - accuracy: 0.7031 - val_loss: 0.5463 - val_accuracy: 0.8025\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 102s 1s/step - loss: 0.5407 - accuracy: 0.8128 - val_loss: 0.4526 - val_accuracy: 0.8375\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 109s 1s/step - loss: 0.4508 - accuracy: 0.8497 - val_loss: 0.4139 - val_accuracy: 0.8487\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 101s 1s/step - loss: 0.3814 - accuracy: 0.8706 - val_loss: 0.4060 - val_accuracy: 0.8575\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 102s 1s/step - loss: 0.3690 - accuracy: 0.8716 - val_loss: 0.3907 - val_accuracy: 0.8562\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 110s 1s/step - loss: 0.3343 - accuracy: 0.8863 - val_loss: 0.3076 - val_accuracy: 0.8788\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 106s 1s/step - loss: 0.3342 - accuracy: 0.8778 - val_loss: 0.3534 - val_accuracy: 0.8775\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 104s 1s/step - loss: 0.2756 - accuracy: 0.9034 - val_loss: 0.2656 - val_accuracy: 0.9162\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 104s 1s/step - loss: 0.2861 - accuracy: 0.8938 - val_loss: 0.2612 - val_accuracy: 0.9125\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 108s 1s/step - loss: 0.2824 - accuracy: 0.9031 - val_loss: 0.3178 - val_accuracy: 0.8988\n",
            "25/25 [==============================] - 14s 546ms/step - loss: 0.3178 - accuracy: 0.8988\n",
            "Training model: efficientnetb0\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 116s 1s/step - loss: 2.3537 - accuracy: 0.0984 - val_loss: 2.3109 - val_accuracy: 0.1000\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 105s 1s/step - loss: 2.3065 - accuracy: 0.0944 - val_loss: 2.3076 - val_accuracy: 0.1000\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 100s 1s/step - loss: 2.3042 - accuracy: 0.0934 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 99s 993ms/step - loss: 2.3034 - accuracy: 0.0941 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 104s 1s/step - loss: 2.3035 - accuracy: 0.0934 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 99s 990ms/step - loss: 2.3028 - accuracy: 0.1000 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 97s 973ms/step - loss: 2.3028 - accuracy: 0.0959 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 98s 983ms/step - loss: 2.3028 - accuracy: 0.0928 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 101s 1s/step - loss: 2.3028 - accuracy: 0.0881 - val_loss: 2.3026 - val_accuracy: 0.1000\n",
            "Epoch 10/10\n",
            " 77/100 [======================>.......] - ETA: 19s - loss: 2.3028 - accuracy: 0.0925"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve class indices mapping\n",
        "class_indices = train_generator.class_indices\n",
        "# Reverse the mapping to get a dictionary that maps indices to class names\n",
        "index_to_class = {v: k for k, v in class_indices.items()}"
      ],
      "metadata": {
        "id": "rjQAVpaKrhys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_class"
      ],
      "metadata": {
        "id": "-FAwX9Wnsy8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsT9rRn8xwFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading the best model"
      ],
      "metadata": {
        "id": "dxRMd0nGtGie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_name):\n",
        "    base_model_map = {\n",
        "        'resnet50': ResNet50,\n",
        "        'vgg16': VGG16,\n",
        "        'inception_v3': InceptionV3,\n",
        "        'mobilenet_v2': MobileNetV2,\n",
        "        'densenet121': DenseNet121,\n",
        "        'efficientnetb0': EfficientNetB0\n",
        "    }\n",
        "    base_model = base_model_map[model_name](weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    model = create_model(base_model)\n",
        "    model.load_weights(f'{model_name}.h5')\n",
        "    return model\n",
        "\n",
        "best_model = load_model(best_model)"
      ],
      "metadata": {
        "id": "h1mqLYWqs2Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction"
      ],
      "metadata": {
        "id": "VNp3R3kstPLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(model, img_path, index_to_class):\n",
        "    from tensorflow.keras.preprocessing import image\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_index = np.argmax(prediction)\n",
        "    predicted_class_name = index_to_class[predicted_index]\n",
        "    return predicted_class_name, prediction\n",
        "\n",
        "# Example usage\n",
        "img_path = '/content/data_set/data/bacterial_leaf_blight/bacterial_leaf_blight1015.jpg'\n",
        "predicted_class_name, probabilities = predict_image(best_model, img_path, index_to_class)\n",
        "print(f'Predicted class: {predicted_class_name} with probabilities: {probabilities}')"
      ],
      "metadata": {
        "id": "0E1F34WqtRHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nd9zIKnoO4V2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}